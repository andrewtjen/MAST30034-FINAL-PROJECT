{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "\n",
    "#### Things to do: \n",
    "fit doc2vec and bert tokenisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit of information on Naiive Bayes (double check this)\n",
    "Naiive Bayes is pretty simple and has no (pretty much no) hyper paramaters to tune. Once trained the model is light weight and quick for prediction. One issue with the algorithm is that it assumes that assumes that data is distributed normally. (note we can use other distributions if we need) Additionally by definition it assumes that all the features are independendent from eachother when calculating posteriors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Ytrain = np.array([1, 1, 2, 2, 1, 21])\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X,Y)\n",
    "gnb_model.score(X,Y)\n",
    "score = cross_validate(gnb_model, Xtrain, Ytrain, cv=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
